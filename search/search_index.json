{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":""},{"location":"#welcome-to-mkdocs","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>In case the requirement file is not attached, please installed the following dependencies manually. </p><pre><code>pip install mkdocs-material\npip install \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"computer-vision-FootFallCam/","title":"Object Detection","text":""},{"location":"computer-vision-FootFallCam/#object-detection","title":"Object Detection","text":""},{"location":"computer-vision-FootFallCam/#1-project-brief","title":"1. Project Brief","text":"<p>At FootFallCam, we provide rich footfall all data for retail stores as well as advanced analytics and insights to help business decision making. Our 3D sensor uses computer vision and complex AI algorithm to track footfall accurately. One of our key features is staff identification.</p> <p>A person is identification as a staff if s/he is wearing a staff name tag, as shown below.</p> <p></p> <p>As an AI Engineer, you have a few hundred of video samples from our 3D sensor. Here, you are given 1 sample (\u201csample.mp4\u201d) which is the view from our 3D sensor. Within the clip, there is a person wearing the tag and walking along the corridor. Below are 2 snapshot instances of the person, all other people are not wearing the tag:</p> <p></p> <p></p> <p>Task</p> <ul> <li>Identify which frames in the clip have the staff present?</li> <li>Locate the staff xy coordinates when present in the clip.</li> </ul>"},{"location":"computer-vision-FootFallCam/#video","title":"Video","text":""},{"location":"computer-vision-FootFallCam/#2-setting-up","title":"2. Setting Up","text":""},{"location":"computer-vision-FootFallCam/#virtual-environment","title":"Virtual Environment","text":"<p>To ensure isolated package management and avoid conflicts, create and activate a virtual environment.</p> <p>Windows OS</p> <pre><code># Create a virtual environment\npython -m venv myenv  # Replace 'myenv' with your desired environment name\n\n# Activate the environment\nmyenv\\Scripts\\activate\n</code></pre> <p>Install Dependencies</p> <p>To install the required libraries, use the <code>requirements.txt</code> file. </p><pre><code># Ensure that your requirements.txt file contains all necessary packages for the project.\npip install -r requirements.txt\n</code></pre>"},{"location":"computer-vision-FootFallCam/#roboflow","title":"Roboflow","text":"<p>Roboflow is a comprehensive platform designed to simplify the process of building and deploying computer vision models. It provides a suite of tools that cater to various stages of a computer vision project.</p> Feature Description Use in This Project Annotation Provides an intuitive interface for annotating images, making it easy to label objects within your dataset for training accurate models. Used for image annotation. Dataset Management Allows efficient organization and management of datasets, supporting various formats and enabling splitting into training, validation, and test sets. Helps in organizing the annotated dataset. Augmentation Includes powerful augmentation tools to simulate different conditions and variations in data, enhancing dataset robustness and model performance. N/A Model Training Integrates with popular machine learning frameworks, offering pre-trained models and the ability to train models directly on the platform. Model is trained using YOLO in Jupyter Notebook. Deployment Simplifies the process of deploying models to different environments, including cloud services and edge devices. N/A <p>Note: For full documentation, please visit Ultralytics.</p>"},{"location":"computer-vision-FootFallCam/#training-model","title":"Training Model","text":"<p>It is recommended to run the notebook in Google Colab, as local execution may lead to various issues. Google Colab offers a stable environment with access to TPU, which can greatly accelerate the training process while minimizing these potential problems.</p> <p>This code initializes the YOLO environment by</p> <ul> <li>Installing necessary packages</li> <li>Importing required modules </li> <li>Clearing the output for a clean workspace.</li> </ul> <pre><code># Code provide by Roboflow\n!pip install ultralytics\n\nfrom ultralytics import YOLO\nimport os\nfrom IPython.display import display, Image\nfrom IPython import display\ndisplay.clear_output()\n</code></pre> <p>Download dataset from Roboflow. </p><pre><code># Sample code for downloading the dataset from Roboflow.\n!pip install roboflow\nfrom roboflow import Roboflow\n\nrf = Roboflow(api_key=\"YOUR_API_KEY\")  # Use your own API key\nproject = rf.workspace().project(\"your-project-name\")\ndataset = project.version(1).download(\"yolov8\")\n</code></pre> <p>Training the model </p><pre><code>!yolo task=detect mode=train model=yolov8n.pt data={dataset.location}/data.yaml epochs=100 imgsz=640 patience=5\n</code></pre> Parameter Description Details <code>!yolo</code> Shell command in Jupyter Notebook or Google Colab that executes the YOLO training process. The <code>!</code> symbol runs shell commands from a Python environment. Used to initiate YOLO training. <code>task=detect</code> Specifies the type of task: \u2022 <code>detect</code> (object detection)  \u2022 <code>segment</code> (segmentation)  \u2022 <code>classify</code> (image classification) <code>detect</code> is used for object detection in this case. <code>mode=train</code> Specifies the operation mode: \u2022 <code>train</code> indicates training mode, \u2022 <code>predict</code> for prediction, \u2022 <code>val</code> for validation. Indicates the model is in training mode. <code>model=yolov8n.pt</code> Defines which pre-trained model to use as the base for training.  \u2022 <code>yolov8s.pt</code> (small) \u2022 <code>yolov8m.pt</code> (medium) \u2022 <code>yolov8l.pt</code> (large) \u2022 <code>yolov8x.pt</code> (extra-large) YOLOv8 is the nano model, a lightweight version designed for faster performance, though it sacrifices some accuracy compared to larger models. <code>data={dataset.location}/data.yaml</code> Specifies the path to the dataset configuration file (<code>data.yaml</code>). <code>dataset.location</code> points to the dataset's location in the environment. Contains paths to training/validation data and labels. <code>epochs=100</code> Sets the number of training epochs  Note: complete passes through the training dataset. Model will be trained for 100 epochs. <code>imgsz=640</code> Defines the input image size (in pixels) for the model during training. Images are resized to 640x640 pixels. <code>patience=5</code> Specifies the number of epochs without improvement to wait before early stopping. Training stops if no improvement after 5 epochs."},{"location":"computer-vision-FootFallCam/#3-overview","title":"3. Overview","text":"<p>This module provides two classes for processing video files and performing object detection.</p> Class Description <code>VideoProcessor</code> Handles video frame extraction and processing using OpenCV. <code>ObjectDetection</code> Leverages YOLO (You Only Look Once) for detecting objects in video files."},{"location":"computer-vision-FootFallCam/#video-processor-class","title":"Video Processor Class","text":"<p>The <code>VideoProcessor</code> class is designed to process video files using OpenCV. It offers functionality to extract frames from the video and display each frame with additional overlay information.</p> <p>Attributes</p> Parameter Description <code>video_path (str)</code> Path to the input video file. <code>output_folder (str)</code> Directory to save the extracted frames. <code>cap</code> OpenCV object to capture video frames. <p>Initialize the VideoProcessor </p><pre><code>processor = VideoProcessor(video_path='input_video.mp4', output_folder='output_frames_folder')\n</code></pre> <p>Extract frames from the video, starting with frame number 0 </p><pre><code>processor.extract_frames()\n</code></pre> <p>Process the video, displaying each frame with an overlay of the frame count </p><pre><code>processor.process_video()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#functionality","title":"Functionality","text":"<p><code>extract_frames(self)</code></p> <p>Extracts frames from the video file and saves them as image files (<code>.jpg</code>). The frames are saved with filenames formatted as <code>frame_0000.jpg</code>, <code>frame_0001.jpg</code>, etc. The extraction continues until the end of the video.</p> <ol> <li> <p>Initializes <code>frame_count</code> with the given <code>count</code> parameter.</p> </li> <li> <p>Continuously reads frames from the video file while the video capture (<code>self.cap</code>) is open.</p> </li> <li> <p>For each frame successfully read (<code>ret</code> is <code>True</code>):</p> <ul> <li>Creates a filename for the frame using the current <code>frame_count</code> (e.g., <code>frame_0000.jpg</code>).</li> <li>Saves the frame as an image file in the specified output folder using <code>cv2.imwrite()</code>.</li> <li>Increments the <code>frame_count</code> for the next frame.</li> <li>If no frame is read (<code>ret</code> is <code>False</code>), the loop breaks, and the extraction process ends.</li> </ul> </li> <li> <p>Releases the video capture object and prints the total number of frames extracted.</p> </li> </ol> <pre><code>def extract_frames(self):\n    \"\"\"\n    Extract frames from the video file and save them as image files.\n    \"\"\"\n    count = 0\n    while self.cap.isOpened():\n        ret, frame = self.cap.read()\n        if not ret:\n            break\n        frame_filename = os.path.join(self.output_folder, f'frame_{frame_count:04d}.jpg')\n        cv2.imwrite(frame_filename, frame)\n        frame_count += 1\n    self.cap.release()\n    print(f'Total frames extracted: {frame_count}')\n</code></pre> <p><code>process_video(self)</code></p> <p>Processes the video file by displaying each frame and overlaying the frame count on the video. The video display can be interrupted by pressing the 'q' key.</p> <ol> <li> <p>Initializes <code>frame_count</code> to <code>0</code>.</p> </li> <li> <p>Continuously reads frames from the video file while the video capture (<code>self.cap</code>) is open.</p> </li> <li> <p>For each frame successfully read (<code>ret</code> is <code>True</code>):</p> <ul> <li>Overlays the current frame count on the frame using <code>cv2.putText()</code>.</li> <li>Displays the frame in a window titled 'Sample-Video' using <code>cv2.imshow()</code>.</li> <li>Optionally, you can move the window position using <code>cv2.moveWindow()</code> (commented out in the code).</li> </ul> </li> <li> <p>Listens for user input:</p> <ul> <li>If the 'q' key is pressed, the loop breaks, and the video display is interrupted.</li> </ul> </li> <li> <p>After the loop ends, releases the video capture object and closes all OpenCV windows using <code>cv2.destroyAllWindows()</code>.</p> </li> </ol> <pre><code>def process_video(self):\n    \"\"\"\n    Process the video file, display each frame, and overlay the frame count.\n    \"\"\"\n    frame_count = 0\n    while self.cap.isOpened():\n        ret, frame = self.cap.read()\n        if not ret:\n            break\n        # Overlay the frame count on the frame\n        cv2.putText(\n            frame, f'Frame: {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n        cv2.imshow('Sample-Video', frame)\n        # cv2.moveWindow('Sample-Video', 0, 0)\n\n        if cv2.waitKey(25) &amp; 0xFF == ord('q'):\n            break\n\n        frame_count += 1\n\n    self.cap.release()\n    cv2.destroyAllWindows()\n</code></pre> <p><code>__del__(self)</code></p> <p>Releases the video capture object and destroys all OpenCV windows when the <code>VideoProcessor</code> object is deleted.</p> <ol> <li>Checks if the <code>VideoProcessor</code> object has a <code>cap</code> attribute and if it is currently opened.</li> <li>If <code>self.cap</code> exists and is open (<code>self.cap.isOpened()</code> returns <code>True</code>), the video capture object is released using <code>self.cap.release()</code>.</li> <li>Closes all OpenCV windows using <code>cv2.destroyAllWindows()</code>.</li> </ol> <pre><code>def __del__(self):\n    \"\"\"\n    Release the video capture and destroy all OpenCV windows.\n    \"\"\"\n    if hasattr(self, 'cap') and self.cap.isOpened():\n        self.cap.release()\n    cv2.destroyAllWindows()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#object-detection-class","title":"Object Detection Class","text":"<p>A class to perform object detection on a video file using a YOLO model.</p> <p>Attributes</p> Parameter Description <code>video_path</code> (str) Path to the input video file. <code>model</code> (YOLO) YOLO model loaded from the specified <code>model_path</code>. Used for performing object detection on video frames. <p>Initializes the <code>ObjectDetection</code> object with the path to the video file and the path to the YOLO model. </p><pre><code># Create an ObjectDetection instance with the video path and model path\ndetector = ObjectDetection('input_video.mp4', 'yolov8n.pt')\ndetector.detect_objects()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#functionality_1","title":"Functionality","text":"<p><code>detect_objects(self)</code></p> <ol> <li> <p>Invokes the object detection model on the specified video or image source (self.video_path).</p> </li> <li> <p>Sets the following parameters for the model's prediction:</p> <ul> <li><code>show=True</code> : Displays the detection results in a window.</li> <li><code>conf=0.4</code> : Uses a confidence threshold of 0.4 for detected objects.</li> <li><code>save=True</code> : Saves the detection results.</li> <li><code>stream=True</code> : Streams detection results in real-time.</li> </ul> </li> <li> <p>Iterates over the detection results for each frame or image.</p> <ul> <li>Extracts bounding boxes using <code>r.boxes</code>.</li> <li>Extracts segmentation masks using <code>r.masks</code>.</li> <li>Extracts class probabilities using <code>r.probs</code>.</li> </ul> </li> <li> <p>Prints \"Object detection completed\" after processing all frames/images.</p> </li> </ol> <pre><code>def detect_objects(self):\n    results = self.model(\n        source=self.video_path, show=True, conf=0.4, save=True, stream=True)\n\n    for r in results:\n        boxes = r.boxes  # Bounding box outputs\n        masks = r.masks  # Segmentation masks outputs\n        probs = r.probs  # Class probabilities for classification outputs\n\n    print(\"Object detection completed.\")\n</code></pre>"},{"location":"computer-vision-FootFallCam/#image-retrieval-class","title":"Image Retrieval Class","text":"<p>A class to identify the image from a dataset that is most similar to the target image.</p> <p>Attributes</p> Attribute Description <code>self.sift</code> Instance of the SIFT feature extractor for detecting key points and computing descriptors. <code>self.flann</code> Instance of the FLANN-based matcher for matching descriptors between images. <code>self.index_params</code> Parameters for FLANN matching (e.g., algorithm and number of trees). <code>self.search_params</code> Search parameters for FLANN matching (e.g., number of checks). <p>Initializes the <code>ImageRetriever</code> object with the path to the images folder and targeted image. </p><pre><code># if __name__ == \"__main__\":\n    target_image_path = 'media\\\\project\\\\white-shirt-staff.png'  # Path to the image you want to find\n    dataset_folder = 'media\\\\extracted_frames'             # Folder containing the sample of 100 images\n\n    # Create an instance of ImageRetriever\n    image_retriever = ImageRetriever()\n    best_match = image_retriever.find_best_match(target_image_path, dataset_folder)\n\n    print(f\"The image most similar to the target is: {best_match}\")\n</code></pre>"},{"location":"computer-vision-FootFallCam/#functionality_2","title":"Functionality","text":"<p><code>extract_sift_features(self, image_path)</code></p> <ol> <li> <p>Read the Image by using OpenCV's <code>imread</code> to load the image in grayscale mode for feature extraction.</p> <ul> <li><code>image_path</code>: Path to the image file.</li> <li><code>cv2.IMREAD_GRAYSCALE</code>: Loads the image in grayscale to simplify data and focus on structure.</li> </ul> </li> <li> <p>Check Image Load Success</p> <ul> <li>If <code>image is None</code>, the image could not be read (e.g., incorrect path).</li> <li>Provides feedback for debugging by indicating the issue with the image path.</li> </ul> </li> <li> <p>Extract Features with by using SIFT (<code>detectAndCompute</code>) to extract feature.</p> <ul> <li><code>keypoints</code>: List of important locations/features detected.</li> <li><code>descriptors</code>: Numerical representation of the keypoints for matching.</li> </ul> </li> <li> <p>Return Features</p> <ul> <li>Returns the extracted keypoints and descriptors as a tuple for further processing.</li> </ul> </li> </ol> <pre><code>def extract_sift_features(self, image_path):\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        raise ValueError(f\"Could not read image at path: {image_path}\")\n    keypoints, descriptors = self.sift.detectAndCompute(image, None)\n    return keypoints, descriptors\n</code></pre> <p><code>match_features(self, descriptors1, descriptors2)</code></p> <ol> <li> <p>Match features between two sets of descriptors using the FLANN-based matcher.</p> <ul> <li>descriptors1: Descriptors of the first image.</li> <li>descriptors2: Descriptors of the second image.</li> </ul> </li> <li> <p>Check Matches</p> <ul> <li>Uses FLANN's <code>knnMatch</code> to find the two closest matches (k=2) for each descriptor in descriptors1.</li> </ul> </li> <li> <p>Filter Good Matches</p> <ul> <li>Applies Lowe's ratio test to filter out poor matches.</li> <li>good_matches: List of matches where the distance of the best match is less than 0.7 times the distance of the second-best match.</li> </ul> </li> <li> <p>Return Matches Count</p> <ul> <li>Returns the number of good matches found.</li> </ul> </li> </ol> <pre><code>def match_features(self, descriptors1, descriptors2):\n    matches = self.flann.knnMatch(descriptors1, descriptors2, k=2)\n\n    # Apply Lowe's ratio test to filter good matches\n    good_matches = [m for m, n in matches if m.distance &lt; 0.7 * n.distance]\n\n    return len(good_matches)\n</code></pre> <p><code>find_best_match(self, target_image_path, dataset_folder)</code></p> <ol> <li> <p>Find the image in the dataset that best matches the target image.</p> <ul> <li>target_image_path: Path to the target image.</li> <li>dataset_folder: Path to the folder containing dataset images.</li> </ul> </li> <li> <p>Extract Features from Target Image</p> <ul> <li>Calls <code>extract_sift_features</code> to get keypoints and descriptors of the target image.</li> <li>target_keypoints: Keypoints of the target image.</li> <li>target_descriptors: Descriptors of the target image.</li> </ul> </li> <li> <p>Initialize Best Match Variables</p> <ul> <li>best_match_count: Counter for the best match found (initially 0).</li> <li>best_match_image: Placeholder for the filename of the best-matching image (initially None).</li> </ul> </li> <li> <p>Iterate Through Dataset Images</p> <ul> <li>For each image file in the dataset folder:</li> <li>Constructs the full path for the image.</li> <li>Calls <code>extract_sift_features</code> to get descriptors of the dataset image.</li> <li>Calls <code>match_features</code> to compare target descriptors with dataset descriptors.</li> <li>If the current match count exceeds the best match count:</li> <li>Update best_match_count and best_match_image with the current image.</li> </ul> </li> <li> <p>Return Best Match</p> <ul> <li>Returns the filename of the best-matching image found in the dataset.</li> </ul> </li> </ol> <pre><code>def find_best_match(self, target_image_path, dataset_folder):\n    target_keypoints, target_descriptors = self.extract_sift_features(target_image_path)\n\n    best_match_count = 0\n    best_match_image = None\n\n    for image_file in os.listdir(dataset_folder):\n        image_path = os.path.join(dataset_folder, image_file)\n\n        _, dataset_descriptors = self.extract_sift_features(image_path)\n\n        match_count = self.match_features(target_descriptors, dataset_descriptors)\n\n        if match_count &gt; best_match_count:\n            best_match_count = match_count\n            best_match_image = image_file\n\n    return best_match_image\n</code></pre>"},{"location":"computer-vision-FootFallCam/#4-script","title":"4. Script","text":"<p>Define the path to the video file, output folder, and model path </p><pre><code>from business_logic import *\n\n\nvideo_path = 'sample.mp4'  # Path to the input video file\noutput_folder = 'extracted_frames'  # Folder to save extracted frames\nmodel_path = 'model'  # Path to the YOLO model\n</code></pre> <p>Create VideoProcessor instance and frame extraction and video processing </p><pre><code>processor = VideoProcessor(video_path, output_folder)  # Create VideoProcessor instance\nprocessor.process_video()  # Display the video with frame count overlay\nprocessor.extract_frames()  # Extract frames and save them as images\n</code></pre> <p>Create ObjectDetection instance and detect objects in the video </p><pre><code>detector = ObjectDetection(video_path, model_path)\ndetector.detect_objects()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#5-results","title":"5. Results","text":"<p>Currently, multiple object detection models are implemented, with the nano version and the larger version being actively used to produce detection results as shown below:</p> <ol> <li> <p>YOLO 8X Model (Human Detection) </p> </li> <li> <p>YOLO 8X Model (Nametag Detection) </p> </li> <li> <p>YOLO 8X Model (Targeted Staff) </p> </li> <li> <p>YOLO 8N Model (Targeted staff with 100 epoch) </p> </li> </ol>"}]}