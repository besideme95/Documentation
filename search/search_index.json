{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":""},{"location":"#welcome-to-mkdocs","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>In case the requirement file is not attached, please installed the following dependencies manually. </p><pre><code>pip install mkdocs-material\npip install \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"computer-vision-FootFallCam/","title":"Object Detection","text":""},{"location":"computer-vision-FootFallCam/#object-detection","title":"Object Detection","text":""},{"location":"computer-vision-FootFallCam/#1-project-brief","title":"1. Project Brief","text":"<p>At FootFallCam, we provide rich footfall all data for retail stores as well as advanced analytics and insights to help business decision making. Our 3D sensor uses computer vision and complex AI algorithm to track footfall accurately. One of our key features is staff identification.</p> <p>A person is identification as a staff if s/he is wearing a staff name tag, as shown below.</p> <p></p> <p>As an AI Engineer, you have a few hundred of video samples from our 3D sensor. Here, you are given 1 sample (\u201csample.mp4\u201d) which is the view from our 3D sensor. Within the clip, there is a person wearing the tag and walking along the corridor. Below are 2 snapshot instances of the person, all other people are not wearing the tag:</p> <p></p> <p></p> <p>Task</p> <ul> <li>Identify which frames in the clip have the staff present?</li> <li>Locate the staff xy coordinates when present in the clip.</li> </ul>"},{"location":"computer-vision-FootFallCam/#video","title":"Video","text":""},{"location":"computer-vision-FootFallCam/#2-setting-up","title":"2. Setting Up","text":""},{"location":"computer-vision-FootFallCam/#virtual-environment","title":"Virtual Environment","text":"<p>To ensure isolated package management and avoid conflicts, create and activate a virtual environment.</p> <p>Windows OS</p> <pre><code># Create a virtual environment\npython -m venv myenv  # Replace 'myenv' with your desired environment name\n\n# Activate the environment\nmyenv\\Scripts\\activate\n</code></pre> <p>Install Dependencies</p> <p>To install the required libraries, use the <code>requirements.txt</code> file. </p><pre><code># Ensure that your requirements.txt file contains all necessary packages for the project.\npip install -r requirements.txt\n</code></pre>"},{"location":"computer-vision-FootFallCam/#roboflow","title":"Roboflow","text":"<p>Roboflow is a comprehensive platform designed to simplify the process of building and deploying computer vision models. It provides a suite of tools that cater to various stages of a computer vision project.</p> Feature Description Use in This Project Annotation Provides an intuitive interface for annotating images, making it easy to label objects within your dataset for training accurate models. Used for image annotation. Dataset Management Allows efficient organization and management of datasets, supporting various formats and enabling splitting into training, validation, and test sets. Helps in organizing the annotated dataset. Augmentation Includes powerful augmentation tools to simulate different conditions and variations in data, enhancing dataset robustness and model performance. N/A Model Training Integrates with popular machine learning frameworks, offering pre-trained models and the ability to train models directly on the platform. Model is trained using YOLO in Jupyter Notebook. Deployment Simplifies the process of deploying models to different environments, including cloud services and edge devices. N/A <p>Note: For full documentation, please visit Ultralytics.</p>"},{"location":"computer-vision-FootFallCam/#training-model","title":"Training Model","text":"<p>It is recommended to run the notebook in Google Colab, as local execution may lead to various issues. Google Colab offers a stable environment with access to TPU, which can greatly accelerate the training process while minimizing these potential problems.</p> <p>This code initializes the YOLO environment by</p> <ul> <li>Installing necessary packages</li> <li>Importing required modules </li> <li>Clearing the output for a clean workspace.</li> </ul> <pre><code># Code provide by Roboflow\n!pip install ultralytics\n\nfrom ultralytics import YOLO\nimport os\nfrom IPython.display import display, Image\nfrom IPython import display\ndisplay.clear_output()\n</code></pre> <p>Download dataset from Roboflow. </p><pre><code># Sample code for downloading the dataset from Roboflow.\n!pip install roboflow\nfrom roboflow import Roboflow\n\nrf = Roboflow(api_key=\"YOUR_API_KEY\")  # Use your own API key\nproject = rf.workspace().project(\"your-project-name\")\ndataset = project.version(1).download(\"yolov8\")\n</code></pre> <p>Training the model </p><pre><code>!yolo task=detect mode=train model=yolov8n.pt data={dataset.location}/data.yaml epochs=100 imgsz=640 patience=5\n</code></pre> Parameter Description Details <code>!yolo</code> Shell command in Jupyter Notebook or Google Colab that executes the YOLO training process. The <code>!</code> symbol runs shell commands from a Python environment. Used to initiate YOLO training. <code>task=detect</code> Specifies the type of task: \u2022 <code>detect</code> (object detection)  \u2022 <code>segment</code> (segmentation)  \u2022 <code>classify</code> (image classification) <code>detect</code> is used for object detection in this case. <code>mode=train</code> Specifies the operation mode: \u2022 <code>train</code> indicates training mode, \u2022 <code>predict</code> for prediction, \u2022 <code>val</code> for validation. Indicates the model is in training mode. <code>model=yolov8n.pt</code> Defines which pre-trained model to use as the base for training.  \u2022 <code>yolov8s.pt</code> (small) \u2022 <code>yolov8m.pt</code> (medium) \u2022 <code>yolov8l.pt</code> (large) \u2022 <code>yolov8x.pt</code> (extra-large) YOLOv8 is the nano model, a lightweight version designed for faster performance, though it sacrifices some accuracy compared to larger models. <code>data={dataset.location}/data.yaml</code> Specifies the path to the dataset configuration file (<code>data.yaml</code>). <code>dataset.location</code> points to the dataset's location in the environment. Contains paths to training/validation data and labels. <code>epochs=100</code> Sets the number of training epochs  Note: complete passes through the training dataset. Model will be trained for 100 epochs. <code>imgsz=640</code> Defines the input image size (in pixels) for the model during training. Images are resized to 640x640 pixels. <code>patience=5</code> Specifies the number of epochs without improvement to wait before early stopping. Training stops if no improvement after 5 epochs."},{"location":"computer-vision-FootFallCam/#3-overview","title":"3. Overview","text":"<p>This module provides two classes for processing video files and performing object detection.</p> Class Description <code>VideoProcessor</code> Handles video frame extraction and processing using OpenCV. <code>ObjectDetection</code> Leverages YOLO (You Only Look Once) for detecting objects in video files."},{"location":"computer-vision-FootFallCam/#video-processor-class","title":"Video Processor Class","text":"<p>The <code>VideoProcessor</code> class is designed to process video files using OpenCV. It offers functionality to extract frames from the video and display each frame with additional overlay information.</p> <p>Attributes</p> Parameter Description <code>video_path (str)</code> Path to the input video file. <code>output_folder (str)</code> Directory to save the extracted frames. <code>cap</code> OpenCV object to capture video frames. <p>Initialize the VideoProcessor </p><pre><code>processor = VideoProcessor(video_path='input_video.mp4', output_folder='output_frames_folder')\n</code></pre> <p>Extract frames from the video, starting with frame number 0 </p><pre><code>processor.extract_frames()\n</code></pre> <p>Process the video, displaying each frame with an overlay of the frame count </p><pre><code>processor.process_video()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#functionality","title":"Functionality","text":"<p><code>extract_frames(self)</code></p> <p>Extracts frames from the video file and saves them as image files (<code>.jpg</code>). The frames are saved with filenames formatted as <code>frame_0000.jpg</code>, <code>frame_0001.jpg</code>, etc. The extraction continues until the end of the video.</p> <ol> <li>Initializes <code>frame_count</code> with the given <code>count</code> parameter.</li> <li>Continuously reads frames from the video file while the video capture (<code>self.cap</code>) is open.</li> <li>For each frame successfully read (<code>ret</code> is <code>True</code>):<ul> <li>Creates a filename for the frame using the current <code>frame_count</code> (e.g., <code>frame_0000.jpg</code>).</li> <li>Saves the frame as an image file in the specified output folder using <code>cv2.imwrite()</code>.</li> <li>Increments the <code>frame_count</code> for the next frame.</li> </ul> </li> <li>If no frame is read (<code>ret</code> is <code>False</code>), the loop breaks, and the extraction process ends.</li> <li>Releases the video capture object and prints the total number of frames extracted.</li> </ol> <pre><code>def extract_frames(self):\n    \"\"\"\n    Extract frames from the video file and save them as image files.\n    \"\"\"\n    count = 0\n    while self.cap.isOpened():\n        ret, frame = self.cap.read()\n        if not ret:\n            break\n        frame_filename = os.path.join(self.output_folder, f'frame_{frame_count:04d}.jpg')\n        cv2.imwrite(frame_filename, frame)\n        frame_count += 1\n    self.cap.release()\n    print(f'Total frames extracted: {frame_count}')\n</code></pre> <p><code>process_video(self)</code></p> <p>Processes the video file by displaying each frame and overlaying the frame count on the video. The video display can be interrupted by pressing the 'q' key.</p> <ol> <li>Initializes <code>frame_count</code> to <code>0</code>.</li> <li>Continuously reads frames from the video file while the video capture (<code>self.cap</code>) is open.</li> <li>For each frame successfully read (<code>ret</code> is <code>True</code>):<ul> <li>Overlays the current frame count on the frame using <code>cv2.putText()</code>.</li> <li>Displays the frame in a window titled 'Sample-Video' using <code>cv2.imshow()</code>.</li> <li>Optionally, you can move the window position using <code>cv2.moveWindow()</code> (commented out in the code).</li> </ul> </li> <li>Listens for user input:<ul> <li>If the 'q' key is pressed, the loop breaks, and the video display is interrupted.</li> </ul> </li> <li>After the loop ends, releases the video capture object and closes all OpenCV windows using <code>cv2.destroyAllWindows()</code>.</li> </ol> <pre><code>def process_video(self):\n    \"\"\"\n    Process the video file, display each frame, and overlay the frame count.\n    \"\"\"\n    frame_count = 0\n    while self.cap.isOpened():\n        ret, frame = self.cap.read()\n        if not ret:\n            break\n        # Overlay the frame count on the frame\n        cv2.putText(\n            frame, f'Frame: {frame_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n        cv2.imshow('Sample-Video', frame)\n        # cv2.moveWindow('Sample-Video', 0, 0)\n\n        if cv2.waitKey(25) &amp; 0xFF == ord('q'):\n            break\n\n        frame_count += 1\n\n    self.cap.release()\n    cv2.destroyAllWindows()\n</code></pre> <p><code>__del__(self)</code></p> <p>Releases the video capture object and destroys all OpenCV windows when the <code>VideoProcessor</code> object is deleted.</p> <ol> <li>Checks if the <code>VideoProcessor</code> object has a <code>cap</code> attribute and if it is currently opened.</li> <li>If <code>self.cap</code> exists and is open (<code>self.cap.isOpened()</code> returns <code>True</code>), the video capture object is released using <code>self.cap.release()</code>.</li> <li>Closes all OpenCV windows using <code>cv2.destroyAllWindows()</code>.</li> </ol> <pre><code>def __del__(self):\n    \"\"\"\n    Release the video capture and destroy all OpenCV windows.\n    \"\"\"\n    if hasattr(self, 'cap') and self.cap.isOpened():\n        self.cap.release()\n    cv2.destroyAllWindows()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#object-detection-class","title":"Object Detection Class","text":"<p>A class to perform object detection on a video file using a YOLO model.</p> <p>Attributes</p> Parameter Description <code>video_path</code> (str) Path to the input video file. <code>model</code> (YOLO) YOLO model loaded from the specified <code>model_path</code>. Used for performing object detection on video frames. <p>Initializes the <code>ObjectDetection</code> object with the path to the video file and the path to the YOLO model. </p><pre><code># Create an ObjectDetection instance with the video path and model path\ndetector = ObjectDetection('input_video.mp4', 'yolov8n.pt')\ndetector.detect_objects()\n</code></pre>"},{"location":"computer-vision-FootFallCam/#4-script","title":"4. Script","text":"<p>Define the path to the video file, output folder, and model path </p><pre><code>from business_logic import *\n\n\nvideo_path = 'sample.mp4'  # Path to the input video file\noutput_folder = 'extracted_frames'  # Folder to save extracted frames\nmodel_path = 'model'  # Path to the YOLO model\n</code></pre> <p>Create VideoProcessor instance and frame extraction and video processing </p><pre><code>processor = VideoProcessor(video_path, output_folder)  # Create VideoProcessor instance\nprocessor.process_video()  # Display the video with frame count overlay\nprocessor.extract_frames()  # Extract frames and save them as images\n</code></pre> <p>Create ObjectDetection instance and detect objects in the video </p><pre><code>detector = ObjectDetection(video_path, model_path)\ndetector.detect_objects()\n</code></pre>"}]}